{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb1da77",
   "metadata": {},
   "source": [
    "# Federated Learning\n",
    "\n",
    "Federated learning is a machine learning paradigm that enables decentralized training of a shared model by multiple clients while preserving data privacy. The main idea behind this new paradigm is that each client trains a local model on its own data and then sends only the model updates to a central server, rather than sending the raw data. This allows the model to be trained on a large amount of data without compromising data privacy.\n",
    "\n",
    "Federated learning was first proposed by Google in 2016 (McMahan et al., 2016) and has since been applied in various fields, such as healthcare (Hard et al., 2018), finance (Yoon et al., 2018), and natural language processing (Li et al., 2020).  For example, federated learning could be used to train a model that can make personalized recommendations for each user without requiring the raw data from each user to be shared with a central server. This is the mechanism by which the model is trained on data, while adhering to data privacy requirements.\n",
    "\n",
    "Federated learning is a machine learning approach that offers numerous advantages over traditional centralized methods. Firstly, by leveraging distributed data stores, federated learning can scale to handle significantly larger datasets. Secondly, it prioritizes data privacy by avoiding the transmission of raw data to a central server. Finally, federated learning enables collaboration among multiple clients, allowing them to jointly train a shared model without compromising the security of their individual data. Overall, these benefits make federated learning a promising approach for machine learning in fields where data privacy is of the utmost importance.\n",
    "\n",
    "Federated learning is a process in which a central server distributes a machine learning model to multiple devices. Each device trains the model on its local data and sends the updated model back to the central server. The central server then aggregates the updates from each device to improve the global model. This process is repeated until the model converges and can generate accurate predictions on new data. The key concepts within this process are:\n",
    "\n",
    "\n",
    "* Client: refers to a device or edge node that holds a local dataset and actively participates in the training of the federated model.\n",
    "* Server: represents the central entity that coordinates the training of the federated model and receives model updates from the clients to aggregate into a new version of the global model.\n",
    "* Federated dataset: the collection of decentralized datasets from different clients that are used to train the federated model through collaborative learning.\n",
    "* Federated model: a machine learning model that is trained on the federated dataset using federated learning to make accurate predictions on new data while preserving the privacy of each client's data.\n",
    "* Federated optimization: refers to the process of training the federated model using the decentralized data and model updates from the clients, which enables the model to generalize better on unseen data while preserving the privacy of the clients.\n",
    "* Aggregation: the process of combining the model updates received from the clients into a new version of the global model. This can be done using various methods such as weighted averaging or other approaches.\n",
    "* Rounds: refer to the number of times a federated model is distributed among clients after performing an aggregation to train the model further. The process is repeated until the model converges and achieves a satisfactory level of accuracy.\n",
    "\n",
    "\n",
    "Federated learning is a relatively new approach, and as such, there are few libraries available that have adapted to it. The main actors in this space are TensorFlow Federated, PySyft, OpenMined, and Flower. Of these, TensorFlow Federated is a notable mention, although it is currently only a theoretical approach, as it does not allow for the deployment of the solution and only simulates the federated space. In contrast, Flower allows for the distribution of federated learning, although the necessary modifications can be somewhat challenging. For this tutorial, we have chosen Flower due to its more user-friendly approach and potential for future use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf254d5",
   "metadata": {},
   "source": [
    "# Introduction to Flower (FLWR)\n",
    "\n",
    "Flower is a Python library that offers tools for implementing the communication and coordination aspects of federated learning. Its design emphasizes ease of use and scalability. It's important to note that Flower is not a learning framework in itself, and as such, it wraps other machine learning frameworks like TensorFlow, PyTorch, or Scikit-learn in the communication layer to enable federated learning.\n",
    "\n",
    "To use Flower for federated learning, you will need to install the library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import flwr as fl\n",
    "except ImportError:\n",
    "    !pip install flwr[simulation] \n",
    "    import flwr as fl  # Import again after installation\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError:\n",
    "    !pip install tensorflow\n",
    "    import tensorflow as tf  # Import again after installation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82ce39",
   "metadata": {},
   "source": [
    "When setting up a simulation environment, it's crucial to use the `[simulation]` keyword with the `!pip install flwr[simulation]` command. This ensures that the necessary dependencies for simulating a federated learning environment, such as the `flwr_simulation` package, are also installed.\n",
    "\n",
    "**Important Note:** In a distributed setup, use `!pip install flwr` on both the server and all client devices to ensure consistency.\n",
    "\n",
    "**Installation Guide:**\n",
    "\n",
    "* For the most up-to-date and comprehensive installation instructions, refer to the official Flower documentation: [Link to official Flower documentation](https://flower.ai/)\n",
    "\n",
    "After installing the `flwr` package, import it into your Python code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a157b7b",
   "metadata": {},
   "source": [
    "FLWR provides a range of classes and functions that you can use to set up a federated learning environment, train and evaluate a model, and implement regular updates to the model. For more information, refer to the FLWR [documentation](https://flower.dev/docs/quickstart-tensorflow.html). Before proceeding, it's important to note that the model you define must be serializable so that it can be sent through the network. Not all models are suitable for federated learning. For this example, we'll be using an Artificial Neural Network (ANN) based on TensorFlow, specifically Keras. These models are generally lightweight and well-suited for serialization compared to some other model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd89deea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model using TensorFlow\n",
    "def generate_ann():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Flatten(input_shape=(32, 32, 3)), #input data. 32x32 color images with 3 channels\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"), #hidden layer\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),#hidden layer\n",
    "            tf.keras.layers.Dense(10, activation=\"softmax\"), #output layer. 10 classes\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c167daad",
   "metadata": {},
   "source": [
    "As we will be using a Deep Learning model defined in TensorFlow, it's recommended to load the data into a `Dataset` class to enable the framework to leverage any available hardware acceleration (such as a GPU on the nodes). However, due to some limitations of the framework to serialize the data, it has to be done manually with the following lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b84ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "NUM_CLIENTS = 5\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def split_index(a, n):\n",
    "    s = np.array_split(np.arange(len(a)), n)\n",
    "    return s\n",
    "\n",
    "\n",
    "# Code to load the dataset\n",
    "def load_datasets(num_clients: int):\n",
    "    # Distribute it to train and test set\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    # Normalize data\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "    x_train, y_train = x_train[:10_000], y_train[:10_000]\n",
    "    x_test, y_test = x_test[:1000], y_test[:1000]\n",
    "\n",
    "    # Randomize the datasets\n",
    "    x_train, y_train = unison_shuffled_copies(x_train, y_train)\n",
    "    x_test, y_test = unison_shuffled_copies(x_test, y_test)\n",
    "\n",
    "    # Split training set into NUM_CLIENTS partitions to simulate the individual dataset\n",
    "    train_index = split_index(x_train, num_clients)\n",
    "    test_index = split_index(x_test, num_clients)\n",
    "\n",
    "    # Split each partition\n",
    "    train_ds = []\n",
    "    val_ds = []\n",
    "    test_ds = []\n",
    "    for cid in range(num_clients):\n",
    "        val_size = len(train_index[cid]) // 10\n",
    "        train_input_data, train_output_data = x_train[train_index[cid]], y_train[train_index[cid]]\n",
    "        val_input_data, val_output_data = train_input_data[:val_size], train_output_data[:val_size]\n",
    "        train_input_data, train_output_data = train_input_data[val_size:], train_output_data[val_size:]\n",
    "        train_dataset = (train_input_data, train_output_data)\n",
    "        val_dataset = (val_input_data, val_output_data)\n",
    "        test_dataset = (x_test[test_index[cid]], y_test[test_index[cid]])\n",
    "        train_ds.append(train_dataset)\n",
    "        val_ds.append(val_dataset)\n",
    "        test_ds.append(test_dataset)\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f754d7f5",
   "metadata": {},
   "source": [
    "Before proceeding, it's crucial to perform a basic test to validate the correctness of our approach. This involves training the model on the dataset of a single client. The purposes are:\n",
    "1. **Checks Model Training Functionality**: It ensures that the model can be successfully trained on the data of an individual client.\n",
    "Identifies\n",
    "2. **Potential Issues Early**: It helps to detect and address any errors or unexpected behavior in the data loading or model training processes before moving on to the more complex federated learning implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe1172",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_ann()\n",
    "\n",
    "model.fit(trainloaders[0][0],trainloaders[0][1], epochs=1, batch_size=32, steps_per_epoch=3)\n",
    "loss, accuracy = model.evaluate(valloaders[0][0], valloaders[0][1])\n",
    "print(f\"The resulting loss is {loss} for an accuracy in test of {accuracy*10:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78c70c",
   "metadata": {},
   "source": [
    "**Important**: *The following example is thought to be executed in a terminal* \n",
    "\n",
    "Starting from that point, that it works on a particular data, the federated processs is going to split this process in several machines and for that it is going to require de definition of a couple of additional elements. Therefore, let's introduce the two pieces of the puzzle: the `Client` and the `Server`. \n",
    "\n",
    "\n",
    "\n",
    "Flower starts a `Server` to coordinate the client devices and perform the orchestration of the model. The server interacts with clients through an interface called `Client`. When the server selects a particular client for training, it sends training instructions over the network. The client receives those instructions and calls one of the Client methods to run your code, which in this case involves training the neural network that we defined earlier.\n",
    "\n",
    "Flower provides a convenient class called NumPyClient, which simplifies the implementation of the Client interface when your workload uses Keras. The [NumPyClient](https://flower.ai/docs/framework/ref-api/flwr.client.NumPyClient.html#flwr.client.NumPyClient) interface defines three methods that can be implemented in the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bde54e",
   "metadata": {},
   "source": [
    "```python\n",
    "#Create a class to contain the details of the client and be the interface\n",
    "class MyClient(fl.client.NumPyClient):\n",
    "    def __init__(self, net, train_dataset, test_dataset):\n",
    "        self.model = net\n",
    "        self.trainloader = train_dataset\n",
    "        self.valloader = test_dataset\n",
    "        \n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        self.model.fit(self.trainloader[0],self.trainloader[1], epochs=1, batch_size=32, steps_per_epoch=3)\n",
    "        return self.model.get_weights(), len(self.trainloader[0]), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, accuracy = self.model.evaluate(self.valloader[0], self.valloader[1])\n",
    "        return loss, len(self.valloader[0]), {\"accuracy\": float(accuracy)}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324ea6a",
   "metadata": {},
   "source": [
    "In the preceding code, we defined the required functions for the client in this particular case. With these functions in place, we can now start a client using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e187591",
   "metadata": {},
   "source": [
    "```python \n",
    "# Start the client\n",
    "model=generate_ann()\n",
    "\n",
    "fl.client.start_client(server_address=f\"localhost:8080\", client=MyClient(model,trainloaders[0],valloaders[0]).to_client())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d96b24",
   "metadata": {},
   "source": [
    "**Important**: In order to run the client you must need also a server running!!! You will executed both in separated terminals\n",
    "\n",
    "The string`localhost:8080` specifies the server to which the client should connect. In this case, as the code is being run on the same machine as the server, this address is sufficient. In a truly federated workload, the only thing that needs to be changed is the `server_address` to point the client to the correct server.\n",
    "\n",
    "Note that Jupyter usually runs on port 8080, **so you will need to use another available port if Jupyter server is running**.\n",
    "\n",
    "\n",
    "The other essential piece of the puzzle is the class that will contain the server. This will be in a separate file, for example, server.py, and its contents should look something like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb282a",
   "metadata": {},
   "source": [
    "```python\n",
    "import flwr as fl\n",
    "\n",
    "fl.server.start_server(config=fl.server.ServerConfig(num_rounds=3))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684abf1",
   "metadata": {},
   "source": [
    "##### **Important**\n",
    "You can use another port for running the server if you fix the following parameter in the `start_server` function: `server_address=\"localhost:8080\"`\n",
    "\n",
    "In this particular case, we can run two clients and a server in separate terminals of the machine. Running two client instances is as simple as executing the `python client.py` command twice in separate terminals, while the server can be started with the `python server.py` command.\n",
    "\n",
    "Upon starting the server, we should receive an output similar to:\n",
    "\n",
    "\n",
    "```shell\n",
    "INFO flwr 2023-03-01 14:58:16,353 | app.py:139 | Starting Flower server, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
    "INFO flwr 2023-03-01 14:58:16,362 | app.py:152 | Flower ECE: gRPC server running (3 rounds), SSL is disabled\n",
    "INFO flwr 2023-03-01 14:58:16,362 | server.py:86 | Initializing global parameters\n",
    "INFO flwr 2023-03-01 14:58:16,362 | server.py:270 | Requesting initial parameters from one random client\n",
    "INFO flwr 2023-03-01 14:58:24,152 | server.py:274 | Received initial parameters from one random client\n",
    "INFO flwr 2023-03-01 14:58:24,153 | server.py:88 | Evaluating initial parameters\n",
    "INFO flwr 2023-03-01 14:58:24,153 | server.py:101 | FL starting\n",
    "DEBUG flwr 2023-03-01 14:58:26,118 | server.py:215 | fit_round 1: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:27,041 | server.py:229 | fit_round 1 received 2 results and 0 failures\n",
    "WARNING flwr 2023-03-01 14:58:27,076 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
    "DEBUG flwr 2023-03-01 14:58:27,076 | server.py:165 | evaluate_round 1: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:27,565 | server.py:179 | evaluate_round 1 received 2 results and 0 failures\n",
    "WARNING flwr 2023-03-01 14:58:27,565 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
    "DEBUG flwr 2023-03-01 14:58:27,566 | server.py:215 | fit_round 2: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:28,015 | server.py:229 | fit_round 2 received 2 results and 0 failures\n",
    "DEBUG flwr 2023-03-01 14:58:28,027 | server.py:165 | evaluate_round 2: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:28,364 | server.py:179 | evaluate_round 2 received 2 results and 0 failures\n",
    "DEBUG flwr 2023-03-01 14:58:28,364 | server.py:215 | fit_round 3: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:28,755 | server.py:229 | fit_round 3 received 2 results and 0 failures\n",
    "DEBUG flwr 2023-03-01 14:58:28,769 | server.py:165 | evaluate_round 3: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:29,184 | server.py:179 | evaluate_round 3 received 2 results and 0 failures\n",
    "INFO flwr 2023-03-01 14:58:29,185 | server.py:144 | FL finished in 5.031599427999936\n",
    "INFO flwr 2023-03-01 14:58:29,185 | app.py:202 | app_fit: losses_distributed [(1, 2.3956351280212402), (2, 2.426431179046631), (3, 2.3015435934066772)]\n",
    "INFO flwr 2023-03-01 14:58:29,185 | app.py:203 | app_fit: metrics_distributed {}\n",
    "INFO flwr 2023-03-01 14:58:29,186 | app.py:204 | app_fit: losses_centralized []\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8f5a6",
   "metadata": {},
   "source": [
    "With that, the first federated learning approach is completed. As you can see, the system goes through three rounds of fitting and evaluating on all clients before the results are retrieved, aggregated, and redistributed to the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa2154-a71c-4ed2-b932-29e26addf00d",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Implement the client and server code in separate files (**All the required files must be submitted together with the Notebook**). Next, execute a server and two clients from terminals. Finally, compare the results with those presented here. Were your results similar?\n",
    "\n",
    "\n",
    "**Note**:  The answer is expected to contain the corresponding terminal output resulting from the execution of your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2140ca-a897-4d2e-817a-2b061a927a85",
   "metadata": {},
   "source": [
    "`Answer here`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf25335",
   "metadata": {},
   "source": [
    "# Updating parameters\n",
    "The key element in this kind of approach is that the server sends the global model parameters to the client, and the client updates the local model with the parameters received from the server. It then trains the model on the local data, which changes the model parameters locally. After training, the updated model parameters are sent back to the server, or alternatively, only the gradients are sent back to the server, not the full model parameters.\n",
    "\n",
    "\n",
    "In `flwr`, this communication is essentially done by two helper functions for loading and retrieving local parameters: `set_parameters` and `get_parameters`. This requirement fits well with non-state approaches such as **PyTorch** or **JAX**. As demonstrated in the previous example, `flwr` can also be used with **TensorFlow** or even **scikit-learn**.\n",
    "\n",
    "As a result, the basic structure for any client using this library has the same format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467be0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# Utility functions for the most common operations\n",
    "def get_parameters(net) -> List[np.array]:\n",
    "    return net.get_weights()\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    net.set_weights(parameters)\n",
    "    return net\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    net.fit(trainloader[0], trainloader[1],\n",
    "            epochs=epochs, batch_size=32, steps_per_epoch=3)\n",
    "    return net\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    loss, accuracy = net.evaluate(testloader[0], testloader[1])\n",
    "    return loss, accuracy\n",
    "\n",
    "# Class to contain a Client\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        self.net = set_parameters(self.net, parameters)\n",
    "        self.net = train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        self.net = set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        print(f\"[Client {self.cid}] loss:{loss}, Client {self.cid} accuracy:{accuracy}\")\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508e352",
   "metadata": {},
   "source": [
    "In Flower, clients can be created by extending either the `flwr.client.Client` or `flwr.client.NumPyClient` classes. In the previous example, we used `NumPyClient` because it is easier to implement and requires less code as a template. Along with the extended class, there are three main methods that need to be implemented:\n",
    "\n",
    "* `get_parameters`: Returns the current local model parameters.\n",
    "* `fit`: Receives model parameters from the server, trains the model parameters on the local data, and returns the (updated) model parameters to the server.\n",
    "* `evaluate`: Receives model parameters from the server, evaluates the model parameters on the local data, and returns the evaluation result to the server.\n",
    "\n",
    "As you can see, the `MyClient` class implemented in the previous example follows this same structure and the diference is the *id* of the client which is stored for later convinience use in accesing the data.\n",
    "\n",
    "\n",
    "#### Be aware: \n",
    "Sometimes, especially when we are simulating multiple clients on a single device, it can be useful to use a function to create the client when it is required. This is particularly important in stateless frameworks, such as PyTorch, which can benefit from a more efficient implementation that creates clients only when they are required for training or evaluation. For example, the following code loads different examples for each client before discarding them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967498e-6ab5-4f86-8b01-ba14d3053528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.common import Context\n",
    "from flwr.client import ClientApp\n",
    "\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    \"\"\"Returns a FlowerClient containing its data partition.\"\"\"\n",
    "    \n",
    "    # Create the model\n",
    "    net = generate_ann()\n",
    "    #get the identification\n",
    "    partition_id = int(context.node_config[\"partition-id\"])\n",
    "    #Take the appropiate part of the dataset\n",
    "    trainloader = trainloaders[int(partition_id)]\n",
    "    valloader = valloaders[int(partition_id)]\n",
    "    #Create and return the Client\n",
    "    return FlowerClient(partition_id, net, trainloader, valloader).to_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa0980-ca68-4d44-a045-d7a4e1480e9d",
   "metadata": {},
   "source": [
    "From versión 1.13 onward, in order to run the simulation it is manadatory that the clients are contained inse a `ClientApp`. This represent each one of the nodes that are going to be used and call later by the simulation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a8089-7553-40a8-882b-2eb846300f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concstruct the ClientApp passing the client generation function\n",
    "client_app = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eeb598",
   "metadata": {},
   "source": [
    "Note that `myClient` cannot be used in the same sense because of the state that it keeps internally through the function `generate_ann`. However, if this state is removed, it can be used in the same waysimilarly the this CLientFlower.\n",
    "\n",
    "The clients are now set up to load, fit, and evaluate. However, we need to integrate the results from the different clients. In Flower terminology, this is known as a strategy, such as the *Federated Average (FedAvg)* strategy. In a first approach, we can use the built-in implementations of the framework, although custom strategies can also be used. Additionally all those elements has to be set in an object in the Flower ecosystem with a `ServerApp` in order to run them.It has a similar signature to `client_fn` but, instead of returning a client object, it returns all the components needed to run the server-side logic in Flower. Let's see an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5ea32-905b-4530-911a-648dde98b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.server import ServerApp, ServerAppComponents\n",
    "\n",
    "num_rounds = 3\n",
    "\n",
    "def server_fn(context: Context):\n",
    "    # instantiate the model\n",
    "    model = generate_ann()\n",
    "    params = get_parameters(model)# The federated model initial parameters\n",
    "    del model # this is not require but it saves a good amount of memory\n",
    "    # Convert model parameters to flwr.common.Parameters\n",
    "    global_model_init = fl.common.ndarrays_to_parameters(params)\n",
    "\n",
    "\n",
    "    # Create FedAvg strategy\n",
    "    strategy = fl.server.strategy.FedAvg(\n",
    "            fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "            fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n",
    "            min_fit_clients=NUM_CLIENTS,  # Never sample less than NUM_CLIENTS clients for training\n",
    "            min_evaluate_clients=NUM_CLIENTS//2,  # Never sample less than NUM_CLIENTS//2 clients for evaluation\n",
    "            min_available_clients=NUM_CLIENTS,  # Wait until all NUM_CLIENTS clients are available\n",
    "            initial_parameters=fl.common.ndarrays_to_parameters(params), # Initial parameters\n",
    "    )\n",
    "\n",
    "    # Define ServerConfig\n",
    "    config=fl.server.ServerConfig(num_rounds=5)\n",
    "\n",
    "    # Return the configuration and strategy for this server\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "# Create Server\n",
    "server_app = ServerApp(server_fn=server_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd740d-9393-4b64-bbff-cabbdcc15300",
   "metadata": {},
   "source": [
    "Now we have the two pieces of the puzzle. Therefore, the simulation engine can be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e386a3-1167-4fb9-8678-81444bd61966",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl.simulation.run_simulation(\n",
    "    server_app=server_app, client_app=client_app, num_supernodes=NUM_CLIENTS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e8e116",
   "metadata": {},
   "source": [
    "This code corresponds to the script running on the server, and it uses the simulation function to test this approach on a single device with the previously mentioned optimization to avoid overloading the device. The code generates NUM_CLIENTS clients and randomly selects all of them (`fraction_fit = 1.0`) to train the model on all of them. After receiving the updates from the clients, the server performs the aggregation strategy before returning the global model to the clients for the next 3 rounds.\n",
    "\n",
    "**Note**: *Be aware that the simulation is a really resource consuming task. You can get some errors and warnings linked to that. There are different [configurations](https://flower.ai/docs/framework/how-to-run-simulations.html) that you can apply to configure the available resources for simulating the process.*\n",
    "\n",
    "One point to highlight is that the framework is not only going to manage the `losses_distributed`, but none of the other metrics. Due to the diverse treatment of those measures, the framework cannot accurately handle the aggregation of these metrics. Users need to tell the framework how to handle and aggregate these custom metrics.\n",
    "\n",
    "The strategy will then call these functions whenever it receives fit or evaluates metrics from clients. The two possible functions are `fit_metrics_aggregation_fn` and `evaluate_metrics_aggregation_fn`. For example, the following code creates the weighted average, and the previous example can be adapted as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from flwr.common import Metrics\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "def server_fn(context: Context):\n",
    "    # instantiate the model\n",
    "    model = generate_ann()\n",
    "    params = get_parameters(model)# The federated model initial parameters\n",
    "    del model # this is not require but it saves a good amount of memory\n",
    "    # Convert model parameters to flwr.common.Parameters\n",
    "    global_model_init = fl.common.ndarrays_to_parameters(params)\n",
    "\n",
    "\n",
    "    # Create FedAvg strategy\n",
    "    strategy = fl.server.strategy.FedAvg(\n",
    "            fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "            fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n",
    "            min_fit_clients=NUM_CLIENTS,  # Never sample less than NUM_CLIENTS clients for training\n",
    "            min_evaluate_clients=NUM_CLIENTS//2,  # Never sample less than NUM_CLIENTS//2 clients for evaluation\n",
    "            min_available_clients=NUM_CLIENTS,  # Wait until all NUM_CLIENTS clients are available\n",
    "            initial_parameters=fl.common.ndarrays_to_parameters(params), # Initial parameters\n",
    "            evaluate_metrics_aggregation_fn=weighted_average,  # put the metric aggregation for the evaluation\n",
    "    )\n",
    "\n",
    "    # Define ServerConfig\n",
    "    config=fl.server.ServerConfig(num_rounds=5)\n",
    "\n",
    "    # Return the configuration and strategy for this server\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "    # Define ServerConfig\n",
    "    config=fl.server.ServerConfig(num_rounds=5)\n",
    "\n",
    "    # Return the configuration and strategy for this server\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "    \n",
    "# Create Server\n",
    "server_app = ServerApp(server_fn=server_fn)\n",
    "\n",
    "fl.simulation.run_simulation(\n",
    "    server_app=server_app, client_app=client_app, num_supernodes=NUM_CLIENTS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7ba45",
   "metadata": {},
   "source": [
    "We will revisit the definition of custom strategies in the following unit to define our own strategy and attempt to minimize some of the challenges that federated learning must address."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52ac8c",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now is your turn, why not you try to run your own architecture with this approach. Beaware of the high requirements when we are in a simulated environment.\n",
    "\n",
    "**Note**: The objective of this exercise is to bring together the various topics covered by this notebook. This cell should contain all the necessary code for autonomous execution using Flower simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce7d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Code to load the dataset\n",
    "import numpy as np\n",
    "\n",
    "NUM_CLIENTS = 5\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def split_index(a, n):\n",
    "    s = np.array_split(np.arange(len(a)), n)\n",
    "    return s\n",
    "\n",
    "\n",
    "def generate_ann():\n",
    "     #TODO\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "# Code to load the dataset\n",
    "def load_datasets(num_clients: int):\n",
    "    # Distribute it to train and test set\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    # Normalize data\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "    x_train, y_train = x_train[:10_000], y_train[:10_000]\n",
    "    x_test, y_test = x_test[:1000], y_test[:1000]\n",
    "\n",
    "    # Randomize the datasets\n",
    "    x_train, y_train = unison_shuffled_copies(x_train, y_train)\n",
    "    x_test, y_test = unison_shuffled_copies(x_test, y_test)\n",
    "\n",
    "    # Split training set into num_clients partitions to simulate the individual dataset\n",
    "    train_index = split_index(x_train, num_clients)\n",
    "    test_index = split_index(x_test, num_clients)\n",
    "\n",
    "    # Split each partition\n",
    "    train_ds = []\n",
    "    val_ds = []\n",
    "    test_ds = []\n",
    "    for cid in range(num_clients):\n",
    "        val_size = len(train_index[cid]) // 10\n",
    "        train_input_data, train_output_data = x_train[train_index[cid]], y_train[train_index[cid]]\n",
    "        val_input_data, val_output_data = train_input_data[:val_size], train_output_data[:val_size]\n",
    "        train_input_data, train_output_data = train_input_data[val_size:], train_output_data[val_size:]\n",
    "        train_dataset = (train_input_data, train_output_data)\n",
    "        val_dataset = (val_input_data, val_output_data)\n",
    "        test_dataset = (x_test[test_index[cid]], y_test[test_index[cid]])\n",
    "        train_ds.append(train_dataset)\n",
    "        val_ds.append(val_dataset)\n",
    "        test_ds.append(test_dataset)\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)\n",
    "\n",
    "\n",
    "#TODO Client, client_fn, Server and simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b411f",
   "metadata": {},
   "source": [
    "# Aggregation\n",
    "\n",
    "To conclude this lesson, let's take a closer look at the key point of these strategies, which is the aggregation algorithm. These algorithms are responsible for combining the updates from the clients to generate the global model, and they are defined in the strategies as we have seen. Generally speaking, there are several types of aggregation that can be used in federated learning (Reddi et. al, 2020).  \n",
    "\n",
    "Here are the different types of aggregation that can be used in federated learning:\n",
    "\n",
    "* Federated averaging (`flwr.server.strategy.FedAvg`): In this approach, each device computes an update to the model parameters based on its local data, and these updates are then averaged together to create the global model. This approach is simple and effective, but it can be sensitive to the size of the updates and the quality of the data on each device.\n",
    "\n",
    "* Federated weighted averaging: This approach is similar to federated averaging, but each device's update is given a different weight based on the size of its data set or the quality of its data. This can help to give more influence to devices with larger or higher-quality data.\n",
    "\n",
    "* Federated averaging with momentum (`flwr.server.strategy.FedAvgM`): This approach is similar to federated averaging, but it incorporates a momentum term in order to smooth out the updates and help the model converge more quickly.\n",
    "\n",
    "* Federated stochastic gradient descent(`flwr.server.strategy.FedAdagrad`): In this approach, each device computes an update to the model parameters based on a small batch of its local data, rather than the entire data set. This can help to reduce the communication overhead and improve the convergence rate of the model.\n",
    "\n",
    "* Federated ADAM (`flwr.server.strategy.FedAdam`): This approach is a variant of federated stochastic gradient descent that uses the ADAM optimization algorithm to adaptively adjust the learning rate based on the gradient and second moment estimates.\n",
    "\n",
    "\n",
    "\n",
    "All of the previously mentioned aggregation methods, except for Federated Weighted Averaging, are implemented in the `flwr` framework and can be used with the different strategies. Additionally, there are other less common aggregation methods that can be employed. The choice of aggregation method will ultimately depend on the specific characteristics of the data and the requirements of the task at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1fc5f",
   "metadata": {},
   "source": [
    "#### References\n",
    "* Hard, A., Konečný, J., McMahan, H. B., Richemond-Barakat, C., Sivek, J. S., & Talwar, K. (2018). Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1812.02903.\n",
    "* Li, Y., Bonawitz, K., & Talwar, K. (2020). Fedprox: An optimizer for communication-efficient federated learning. arXiv preprint arXiv:2002.04283.\n",
    "* McMahan, H. B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2016). Communication-efficient learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629.\n",
    "* Yoon, J., Hard, A., Konečný, J., McMahan, H. B., & Sohl-Dickstein, J. (2018). Federal regression: A simple and scalable method for heterogeneous federated learning. arXiv preprint arXiv:1812.03862.\n",
    "* Reddi, S., Charles, Z., Zaheer, M., Garrett, Z., Rush, K., Konečný, J., Kumar, S. and McMahan, H.B., 2020. Adaptive federated optimization. arXiv preprint arXiv:2003.00295."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
