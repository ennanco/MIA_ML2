{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b04a600",
   "metadata": {},
   "source": [
    "# Reading data from  a file\n",
    "\n",
    "\n",
    "When working with online learning, data is expected to arrive and be processed by our model as a stream. However, it  is typical to use file-based  datasets to develop our base  model before deploying into an online  production environment. In the previous Labs, the examples we studied used datasets already included in  the River library. Because these datasets are provided by the library developers, they illustrate ideally the characteristics of the algorithms and methods desired of the example. However, we will usually need to load our own datasets. River provides different data connector modules to obtain data from different source types, including CSV files.   To create a proactive datastream from these files,  the River connectors produce generator objects that, when used in an iterator, act as data streams to feed our online models.\n",
    "\n",
    "\n",
    "In this example,  we shall obtain data from a CSV file, exploring different aspects of the process. For this,  the  well-known Titanic dataset will be used.  It can be downloaded from the [Kaggle](https://www.kaggle.com/c/titanic/data) repository.  Here, we shall focus primarily on data input and processing of this data file, so that the model accuracy and performance are secondary.\n",
    "\n",
    "It must be noted that many other preprocesing operations can be done to enhance the performance of a machine learning model in predicting survival chances aboard the Titanic. This notebook only shows some examples in order to demonstrate how to apply different operations with River\n",
    "\n",
    "Even though the online learning scenario differs from that of batch learning, we still need to know input data structure in order to adapt it to our model.  The data structure of the Titanic dataset in the form of a dictionary can be found on the repository (see [Kaggle web page](https://www.kaggle.com/c/titanic/data)).\n",
    "\n",
    "\n",
    "### First step:  \n",
    "\n",
    "Load the dataset with Pandas to check data fields and characteristics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_path=\"./datasets/titanic.csv\" #you must change this var whether the dataset is in another path\n",
    "data=pd.read_csv(dataset_path)\n",
    "print(data.head())\n",
    "print(data.dtypes)\n",
    "print(data.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9969997",
   "metadata": {},
   "source": [
    "Although  River already has a connector to iterate from Pandas DataFrame, for illustrative purposes,  here we shall use [one](https://riverml.xyz/0.14.0/api/stream/iter-csv/) that is  linked to CSV files.\n",
    "\n",
    "<code>iter_csv</code> creates a Python iterator. Each time we use `next` we get a new observation consisting  of a set of both inputs and outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stream\n",
    "from rich import print\n",
    "titanic = stream.iter_csv(dataset_path)\n",
    "sample, target = next(titanic)\n",
    "print(sample)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb52af7",
   "metadata": {},
   "source": [
    "There is no way to know which sample attributes are targets, so each one is loaded as an input. However, the  target attribute names must be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cae8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = stream.iter_csv(dataset_path, target=\"Survived\")\n",
    "sample, target = next(titanic)\n",
    "print(\"###Inputs###\")\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}={value}\")\n",
    "print(\"###Targets###\")\n",
    "print(f\"Survived: {target} - {'yes' if target==1 else 'no'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa7c24",
   "metadata": {},
   "source": [
    "## Numerical and categorical (nominal) attributes\n",
    "\n",
    "When using a River connector, by default, all the loaded attributes are interpreted as strings (categorical attributes).   Therefore, `converters` [parameters](https://riverml.xyz/0.21.0/api/stream/iter-csv/) are used to modify the behavior and assign a specific type to each attribute, referred to as casting. Also,  (lambda) functions can be used to  *convert* the value of the attributes\n",
    "\n",
    "\n",
    "* converters.  Type &rarr; dict | None\n",
    "\n",
    "All values in the CSV are interpreted as strings by default. You can use this parameter to cast values to the desired type. This should be a dict mapping feature names to callables used to parse their associated values. Note that a callable may be a type, such as float and int.\n",
    "    \n",
    "Note that the `convert` functions are executed for each iterator call. If there is a problem in the data conversion of a specific attribute (for example the field is null and cannot be cast), it will produce an exception error.  Considering that data is not loaded at the beginning, but one by one by the iterator, exceptions could be thrown at any moment in the loop.   \n",
    "\n",
    "For example, consider the following:  a `float` converter is assigned to the type of the *Age* attribute, however, in the Titanic dataset,  there are some records that don’t contain *Age* information.  In this case,  since a null cannot be cast to float, a ValueError exception will be throw when an empty *Age* value appears.  For this, a function (`float_converter`) for dealing with bad type conversions is used.  Another possibility is to use the `drop_nones` parameter to remove the attributes with None values that we inserted during the data type conversion process.\n",
    "\n",
    "Often, some fields in a dataset are not useful for the purpose of predictions. Thus, they don’t contain useful information to improve a model's performance.   In these cases,  they can be dropped when the dataset is loaded or removed in future pipeline steps.   This is done with the following command: \n",
    "\n",
    "\n",
    "* drop. Type &rarr; List[str] | None\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_attributes=['PassengerId', 'Name', 'Ticket','Fare',]\n",
    "sex=lambda  g: 0 if g==\"male\" else 1\n",
    "def float_converter(a):\n",
    "    try:\n",
    "        a=float(a)\n",
    "    except ValueError:\n",
    "        a=None\n",
    "    return a\n",
    "\n",
    "cabin=lambda  g: 1 if g!=\"\" else 0 #return 1 if the passenger had a cabin. Otherwise 0\n",
    "    \n",
    "\n",
    "\n",
    "titanic = stream.iter_csv(dataset_path, target=\"Survived\", drop_nones=True,\\\n",
    "                          converters={'Survived':int,'Pclass':int, 'Age':float_converter, 'SibSp':int,'Parch':int, 'Sex':sex, 'Cabin':cabin},\\\n",
    "                          drop=removed_attributes)\n",
    "\n",
    "\n",
    "\n",
    "for sample, target in titanic:\n",
    "    print(sample, 'Survived' if target==1 else 'Deceased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a32f61c",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "\n",
    "Often, a problem can be solved using different machine learning model techniques (eg. SVM, Random Forest, etc).  However, not all the ML models can directly deal with categorical attributes. A typical solution employed is the one-hot attribute encoding. \n",
    "\n",
    "For this, River provides a preprocessing function to online encode categorical attributes. A very powerful feature is that this method does not need to know all the categories in advance;  instead, they are added “on the fly”.\n",
    "\n",
    "An example with the Titanic dataset that can use this one-hot encoding funcion is the *Embarked* attribute: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68349d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stream, compose, preprocessing\n",
    "titanic = stream.iter_csv(dataset_path, target=\"Survived\", drop_nones=True,\\\n",
    "                          converters={'Survived':int,'Pclass':int, 'Age':float_converter, 'SibSp':int,'Parch':int, 'Sex':sex, 'Cabin':cabin},\\\n",
    "                          drop=removed_attributes)\n",
    "\n",
    "\n",
    "\n",
    "pp = compose.Select('Embarked') | preprocessing.OneHotEncoder(drop_zeros=False)\n",
    "for sample, target in titanic:\n",
    "    pp.learn_one(sample)\n",
    "    print(pp.transform_one(sample))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc8a509",
   "metadata": {},
   "source": [
    "#### Adding preprocessing operations to the pipeline\n",
    "\n",
    "Now, we can put many steps together.  For this, we can create a River pipeline containing different preprocessing operations that prepare the data fields that will be inputs to the machine learning online model.   Thus, these steps will include feature engineering, one-hot encoding, feature removing, etc.\n",
    "\n",
    "An example is shown below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a00ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stream, compose, preprocessing\n",
    "removed_attributes=['PassengerId', 'Name', 'Ticket','Fare',]\n",
    "sex=lambda  g: 0 if g==\"male\" else 1\n",
    "def float_converter(a):\n",
    "    try:\n",
    "        a=float(a)\n",
    "    except ValueError:\n",
    "        a=None\n",
    "    return a\n",
    "\n",
    "cabin=lambda  g: 1 if g!=\"\" else 0 #return 1 if the passenger had a cabin. Otherwise 0\n",
    "\n",
    "def generate_new_attributes(x):#feature enginnering\n",
    "    #This feature generation can be improved. Many other interesting features can be developed\n",
    "    x[\"FirstClass\"]=1 if x[\"Pclass\"]==1 else 0\n",
    "    x[\"FamilyMembers\"]=x[\"SibSp\"]+x[\"Parch\"]+1\n",
    "    return x\n",
    "\n",
    "titanic = stream.iter_csv(dataset_path, target=\"Survived\", drop_nones=True,\\\n",
    "                          converters={'Survived':int,'Pclass':int, 'Age':float_converter, 'SibSp':int,'Parch':int, 'Sex':sex, 'Cabin':cabin},\\\n",
    "                          drop=removed_attributes)\n",
    "\n",
    "to_discard=['SibSp','Parch','Embarked',\"Pclass\"]#feature removing\n",
    "\n",
    "pp=compose.FuncTransformer(generate_new_attributes)\\\n",
    "            +(compose.Select('Embarked') | preprocessing.OneHotEncoder(drop_zeros=False))\\\n",
    "            |compose.Discard(*to_discard)\n",
    "                            \n",
    "    \n",
    "\n",
    "for sample, target in titanic:\n",
    "    pp.learn_one(sample)\n",
    "    print(pp.transform_one(sample), 'Survived' if target==1 else 'Deceased')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a9c2e",
   "metadata": {},
   "source": [
    "#### Wrapping up\n",
    "\n",
    "The following includes all the previous steps together with a ML model.  Remember, the purpose of this example is to illustrate data loading and attribute casting.  The model is secondary here and would require more effort to improve its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb56e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stream, compose, preprocessing, evaluate, metrics\n",
    "from river.tree import HoeffdingTreeClassifier\n",
    "removed_attributes=['PassengerId', 'Name', 'Ticket','Fare',]\n",
    "sex=lambda  g: 0 if g==\"male\" else 1\n",
    "def float_converter(a):\n",
    "    try:\n",
    "        a=float(a)\n",
    "    except ValueError:\n",
    "        a=None\n",
    "    return a\n",
    "\n",
    "cabin=lambda  g: 1 if g!=\"\" else 0 #return 1 if the passenger had a cabin. Otherwise 0\n",
    "\n",
    "def generate_new_attributes(x):#feature enginnering\n",
    "    x[\"FirstClass\"]=1 if x[\"Pclass\"]==1 else 0\n",
    "    x[\"FamilyMembers\"]=x[\"SibSp\"]+x[\"Parch\"]+1\n",
    "    return x\n",
    "\n",
    "titanic = stream.iter_csv(dataset_path, target=\"Survived\", drop_nones=True,\\\n",
    "                          converters={'Survived':int,'Pclass':int, 'Age':float_converter, 'SibSp':int,'Parch':int, 'Sex':sex, 'Cabin':cabin},\\\n",
    "                          drop=removed_attributes)\n",
    "\n",
    "to_discard=['SibSp','Parch','Embarked',\"Pclass\"]#feature removing\n",
    "\n",
    "model=compose.FuncTransformer(generate_new_attributes)\\\n",
    "            +(compose.Select('Embarked') | preprocessing.OneHotEncoder(drop_zeros=False))\\\n",
    "            |compose.Discard(*to_discard)\n",
    "\n",
    "model|= HoeffdingTreeClassifier(grace_period=50)\n",
    "\n",
    "\n",
    "print(evaluate.progressive_val_score(dataset=titanic, model=model, metric=metrics.Accuracy()))\n",
    "model['HoeffdingTreeClassifier'].draw()\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa14b6",
   "metadata": {},
   "source": [
    "# Concept Drift\n",
    "\n",
    "As seen so far in our examples, stream learning requires models that can adapt over time and be trained in the progressive validation schema. This is based upon the assumption that most data can change over time. Classical machine learning models often assume that data has a stationary probability distribution, ie. one that is fixed.  However, due to its inherent nature,  stream learning often deals with data whose distribution can change. This phenomenon of changing data in the context of ML has given rise to a new term called **Concept drift**.\n",
    "\n",
    "Concept drift can be classified into two main groups, virtual and real. In virtual concept drift, the distribution of the features changes whereas the relationship between features and target remains stable. Thus, given that  $X$ is the feature set and $y$ is the desired output, changes in $P(X)$ would be observed  but not the joint distribution $P(X,y)$, which would remain the same. On the other hand, in real concept drift, it is the joint probability, $P(X,y)$, that would vary. Such changes could be abrupt or gradual.\n",
    "\n",
    "Several situations exemplify the potential occurrence of concept drift,  including, stock market pricing over time, electricity demand demand over time, or movie preferences of users on sites such as Netflix.  In each case, the abrupt change in the value of a parameter over time is readily imagined.  Consider, for example,  prediction systems used for stocking supermarket inventory.  If this model was trained at the beginning of the 2020 pandemic, which was biased towards face masks, hydroalcoholic gel and toilet paper,  the model would be outdated now.  Thus, over time the preferences of purchases changed and the systems should adapt to this point.\n",
    "\n",
    "\n",
    "The most common approach to deal with the concept drift is the use of drift-aware callback methods that trigger mitigation mechanisms. The main objective is to raise an alarm, and/or execute some callback code, when a change in prediction performance is detected. Some methods rely upon desirable properties for the detectors, such as maximizing the number of True Positives (TP) while simultaneously retaining the (FP) to a minimum, i.e., with high precision. Additionally, such alarm methods should be efficient,  and theoretically, able to work with an infinite data stream.\n",
    "\n",
    "Here an example composed by 5 distributions of 5,000 samples with the following properties is given to explore these ideas. \n",
    "\n",
    "* $dist_a:\\mu=0.9, \\sigma=0.01$\n",
    "* $dist_b:\\mu=0.8, \\sigma=0.05$\n",
    "* $dist_c:\\mu=0.4, \\sigma=0.02$\n",
    "* $dist_d:\\mu=0.6, \\sigma=0.1$\n",
    "* $dist_e:\\mu=0.6, \\sigma=0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc256c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# Generate data for 5 distributions\n",
    "random_state = np.random.RandomState(seed=41)\n",
    "dist_a = random_state.normal(0.9, 0.1, 5000)\n",
    "dist_b = random_state.normal(0.8, 0.05, 5000)\n",
    "dist_c = random_state.normal(0.4, 0.02, 5000)\n",
    "dist_d = random_state.normal(0.6, 0.1, 5000)\n",
    "dist_e = random_state.normal(0.6, 0.01, 5000)\n",
    "\n",
    "# Create out synthetic data stream\n",
    "stream = np.concatenate((dist_a, dist_b, dist_c, dist_d, dist_e))\n",
    "\n",
    "# Auxiliary function to analyse the data\n",
    "def plot_data(stream, dists, drifts=None):\n",
    "    fig = plt.figure(figsize=(7,3), tight_layout=True)\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1])\n",
    "    ax1, ax2 = plt.subplot(gs[0]), plt.subplot(gs[1])\n",
    "    ax1.grid()\n",
    "    ax1.plot(stream, label='Stream')\n",
    "    ax2.grid(axis='y')\n",
    "    for id, dist in enumerate(dists):\n",
    "        ax2.hist(dist, label=f'dist_{id}')\n",
    "    if drifts is not None:\n",
    "        for drift_detected in drifts:\n",
    "            ax1.axvline(drift_detected, color='red')\n",
    "    plt.show()\n",
    "\n",
    "plot_data(stream, [dist_a, dist_b, dist_c, dist_d, dist_e])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0f5f9",
   "metadata": {},
   "source": [
    "Here, we study an example of a concept drift detector, called Adaptative WINdowing (ADWIN). The objective of this method is to determine when a change in the properties of the data stream occurs, by using a moving window that adapts to variations.  As such, it seeks to detect variations at different scales.  \n",
    "\n",
    "The method is based upon a sliding window that monitors the statistics of incoming data and detects changes in the distribution over time. The algorithm maintains two sets of statistics: the statistics of the current window and the statistics of the historical data. When a change in the distribution is detected, the current window is updated and the historical data is forgotten. The size of the window is adaptively adjusted based on the rate of change in the data. The ADWIN algorithm is computationally efficient, with a time complexity of O(log n) per data point, where n is the number of data points processed so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import drift\n",
    "\n",
    "drift_detector = drift.ADWIN()\n",
    "drifts = []\n",
    "\n",
    "for i, val in enumerate(stream):\n",
    "    drift_detector.update(val)   # Data is processed one sample at a time\n",
    "    if drift_detector.drift_detected:\n",
    "        # The drift detector indicates after each sample if there is a drift in the data\n",
    "        print(f'Change detected at index {i}')\n",
    "        drifts.append(i)\n",
    "\n",
    "plot_data(stream, [dist_a, dist_b, dist_c, dist_d, dist_e], drifts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4105c21",
   "metadata": {},
   "source": [
    "As we can see, this method has  identified 3 out of 4 drifts.   Note, that if the change is gradual,  the window is larger and thus,  requires more steps to identify that change. In this particular case, it will require nearly 500 samples to become aware of the change in the data.\n",
    "\n",
    "However, ask yourself:  why hasn’t the method raised an alarm in the last change? The answer is because it is not a drift.   Recall that a drift is a change in the relation between data and target;  in this case,  there is no such because the average is the same. This raises an important caveat for such detectors:  they can only be used for univariate data. That is the reason why they are used to monitor a model's performance and not the data itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5ce543",
   "metadata": {},
   "source": [
    "### Play with River\n",
    "\n",
    "There are other drift detectors that can be used.  Two such detectors that you can study and test with the same dataset are the following: \n",
    "* [The Kolmogorov-Smirnov Windowing method](https://riverml.xyz/latest/api/drift/KSWIN/) \n",
    "* [The Page-Hinkley method](https://riverml.xyz/latest/api/drift/PageHinkley/) \n",
    "\n",
    "Care must be used with the first of these since it is very sensitive to the alpha parameter. Try it with $10^{-5}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b355227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play with River\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c51d6",
   "metadata": {},
   "source": [
    "### Concept Drift based in the error rate\n",
    "\n",
    "Other concept drift detection methods exist that are based on the learning model premise, namely, that the learner's error rate will decrease as the number of analyzed samples increase, as long as the data distribution is stationary.   In this case,  if the algorithm detects that the increase in the error rate exceeds a certain threshold, a warning or callback is triggered. \n",
    "\n",
    "Given that these methods are based on the error rate, we can simulate a sequence of prediction results, where **'1' means that the model failed and '0' that the model was successful**. In a real world problem this information would be directly obtained from the model results once it gets the true label. \n",
    "\n",
    "Next, by using our simulated stream, we can test the **Drift Detection Method** (DDM), that is part of these binary methods.\n",
    "\n",
    "The DDM method has two thresholds. Once the stream of results exceeds the first threshold, the algorithm will warn the user that a change may occur in the near future (warning zone). Once the second threshold is surpassed the algorithm confirms the change and it resets its memory in order to continue detecting new changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1433f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are simulating the performance's model decay over the time. There are 2 abrupt drifts \n",
    "import random \n",
    "random.seed(1234)\n",
    "good_results=random.choices([0,1],  weights=[0.8, 0.2], k=1000) #Return a k sized list of elements chosen from the population [0,1] with replacement\n",
    "bad_results1=random.choices([0,1], weights=[0.6, 0.4], k=1000)#The elements from the list are chosen with different probability\n",
    "bad_results2=random.choices([0,1], weights=[0.2, 0.8], k=1000)\n",
    "data_stream=good_results+bad_results1+bad_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ba3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import drift\n",
    "ddm=drift.binary.DDM(warm_start=100)#We are waiting the first 100 observations before starting to analyze the drif\n",
    "\n",
    "#We simulate the data stream with a loop\n",
    "trigger_warning=True\n",
    "for idx, result in enumerate(data_stream):#The enumerate methods gets and enumerate each sequence value\n",
    "    ddm.update(result)\n",
    "    if ddm.warning_detected and trigger_warning:\n",
    "        print(f\"Warning detected at index {idx}\")\n",
    "        trigger_warning=False#once the threshod is overpassed, it is usually overpassed  during many iterations\n",
    "    if ddm.drift_detected:\n",
    "        print(f\"A drift was detected at index {idx}\")\n",
    "        trigger_warning=True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ec92fb",
   "metadata": {},
   "source": [
    "### Play with River\n",
    "There are other 3 binary drift detection methods in River.  You should study each and test them with the datastream: \n",
    "* [EDDM](https://riverml.xyz/latest/api/drift/binary/EDDM/)\n",
    "* [HDDM_A](https://riverml.xyz/latest/api/drift/binary/HDDM-A/)\n",
    "* [HDDM_W](https://riverml.xyz/latest/api/drift/binary/HDDM-W/)\n",
    "\n",
    "Note, the interface for each method is exactly the same, so you can re-use the code or work with several detectors in parallel. Nonetheless, drift detectors rely upon internal hyperparameters, so their default configurations may not be the optimal for your problem.\n",
    "\n",
    "\n",
    "Warning: the parameters to initialize each detector are different. You should check the available parameters carefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e235635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play with River\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
