{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c81d538",
   "metadata": {},
   "source": [
    "# Introduction to Idea of Stream Learning\n",
    "\n",
    " Time is a key feature of most human activities of interest. Whether it is the time-evolution of the stock market and economy, the dynamics of social networks and other complex systems, or the sensorial input to robots,  the flow of time is an important factor for system understanding and decision making.  With this flow of time, information updates and evolves.  To these changes, we adapt to the new information, interpreting its meaning and modifying if necessary our underlying assumptions.  Thus, real-time AI systems must be able to adapt in a similar manner if they are to be effective.   \n",
    " \n",
    "Continuous time-based data is thought of as “streams”.  AI learning algorithms that can adapt to  these streams “incrementally” have given rise to a new paradigm, referred to as real-time stream- or incremental- learning.  Because real systems are characterized by phenomena that can occur across short or long time scales, such systems can analyze information on multiple scales.  As an example, short term behavior could be the fluctuations of temperature in a datastream over the course of a day, whereas the long-term behavior could consider average behavior of temperatures over decades.  \n",
    "\n",
    "Thus, How could an AI system provide accurate predictions over these two radically different timescales?  More importantly,  how could an AI system understand that the incoming stream is continually evolving and the underlying phenomena may be due to new trends that were not present in the original training data?  These questions are precisely those that incremental or stream learning systems address. \n",
    "\n",
    "### Comparison with classical ML\n",
    "\n",
    "To better understand stream or incremental learning, it is useful to compare it to a traditional machine learning approach. As an  example, consider a time-based signal such as a temperature sensor, consisting of two data items:  the time-stamp with the corresponding amplitude (some scaled voltage, correlated to temperature).   A classical machine learning algorithm could obtain multi-temporal information by first analyzing the short-term relationship of the signal;  this is done by using a sliding window, and then analyzing features of the signal in each of these sliding windows.   With larger windows, and past data, the classical algorithm could study longer temporal structure.   \n",
    "\n",
    "In this way, a classical machine learning approach would usually build up a dataset for training and testing.  This dataset would have several sections of the signal corresponding to features and outputs on different temporal scales, providing some amount of predictability for both short and long term future events.  \n",
    "Immediately, however, the shortcomings of this approach become evident.   First and foremost, predictions are more complicated because the system must distinguish the different scales (and the boundaries of such scales). If the system must make a prediction on a single input data point, how is such multi-scale information incorporated into the decision algorithm?  In other words, the long-term evolution of the input does not appear on a single datapoint,  and thus incorporating a longer-term prediction of an underlying change is more difficult to capture.\n",
    "\n",
    "As a result,  classical machine learning approaches for multi-temporal prediction tend to be ad-hoc and rely upon specially designed ansatz, particularly for adapting to the evolution of the underlying phenomena, rendering past training incomplete or inaccurate.   Therefore,  the common approach to tackle this issue is to periodically retrain and redeploy the ML model.  In some systems, such model updates are performed on short time scales, such as minutes or even seconds. \n",
    "\n",
    "### The New Paradigm: \n",
    "\n",
    "To effectively deal with the temporal flows and the associated problems described above, stream and incremental learning (previously called online learning).  These are specific AI models that are designed to handle continual (and often infinite) data  streams.  With each datapoint that the such systems receive, they continually adjust the (ML) prediction model, instead of using static (or large batch) datasets. \n",
    "While the difference between stream and classical learning may seem subtle, there are several structural technical differences that are important to consider.   These lectures describe such issues and methods for implementing incremental or stream learning. \n",
    "\n",
    "Additionally, this approach also comes with new opportunities that are going to be covered in the following sections as long as the problems.\n",
    "\n",
    "However, before starting, we shall provide some  formal definitions and some background concepts.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b9fb515",
   "metadata": {},
   "source": [
    "## Data Streams\n",
    "\n",
    "At the core of stream learning  is the term “data stream”, which is defined as the transmission sequential collection of coherent individual elements.  In the context of information to be processed by a machine learning algorithm (or system),  this data stream normally consists of data (or meta-data) that can be used to produce a  set of features that are measured simultaneously upon reception. Such data are often time-dependent or time-correlated with a time-stamp.\n",
    "\n",
    "An observation (or sample) is defined as the set of features measured at a particular instant in time. Such samples can have a stable (data) structure;  that is, in each sample the same parameters are available to be included in the measurement/ model,  or the data structure of the sample can be “dynamic”, or  flexible, where the parameters  appear intermittently over time.\n",
    "\n",
    "Thus, in the context of information processing,  we understand a data stream as a continuous set of sample data over time.\n",
    "\n",
    "\n",
    "\n",
    "### Reactive and proactive data streams\n",
    "\n",
    "\n",
    "Data streams can be classified into two types, reactive and proactive,  depending on the relationship with the user.\n",
    "\n",
    "Reactive data streams are those where the system “receives” the data from a producer.   A typical example is that of a website that produces streams: such as the 'X' Data stream Server.  From the interaction with the  endpoint of the website, the datastream is obtained and can be used for downstream processing.  In this case, the form in which the data stream is produced is not in the control of the processing system; an analysis system such as “stream learning” is merely a receiver and has no influence or control further than receiving and reading it. In this sense,  the stream learner and predictor “reacts” to the input stream.  \n",
    "\n",
    "Proactive data streams are those for which the analysis system (i.e., the stream learner/predictive engine)  controls how and when the data stream is obtained or received.  For example, the moment and manner by which data is read from a file can be controlled;  the analysis engine could decide at which speed to read the data, in what order, etc.\n",
    "\n",
    "\n",
    "\n",
    "## Online processing\n",
    "\n",
    "The concept of “online” refers to the processing of a data stream by observation. Due to some confusions of terminology from that used in with the concept of “online education”, the term is now often superseded by the term “incremental”.  Nonetheless, in the specific case of machine learning, it refers to training a model by learning one sample at a time, and subsequently updating training weights appropriately.\n",
    "\n",
    "In this way,  stream learning is quite different from traditional ML approaches that work on batches of data samples. As mentioned previously, when handling data streams, such standard ML methods must often adjust the model after processing new data to account for disparity with the previous underlying behavior, which may have rapidly become obsolete. \n",
    "\n",
    "This change of methodology comes with new computational and technical requirements.  For example, since data samples are handled one at a time in online processing, libraries such as NumPy and PyTorch have significant overhead because they have been optimized to handle batches with vectorization.   In the online ML approach,  past data samples are not processed again, therefore such vectorization is not necessary.  \n",
    "\n",
    "Thus, the online learning model is a stateful and dynamic process. It represents a new machine-learning paradigm.   Like all technical methods,  it has its associated pros and cons when applied  to real world problems. \n",
    "\n",
    "\n",
    "## Datasets in training\n",
    "\n",
    "In most real-world applications in production, approximately 90% of all data streams are  reactive.   However, for building, training, and evaluating ML model performance,  a dataset is built that simulates the behavior of the data stream (as we shall see,  the river application handles this elegantly by making the entire dataset a python generator object).  Once this is done,  the real-time data stream is a mere drop-in replacement of the development version that is based upon a monolithic file. \n",
    "\n",
    "Apart from this, there is a fundamental difference in the way that traditional ML and stream ML train models with the data.  In traditional ML, we know that a data set is normally split into training and evaluation. In the case of stream learning,  the online approach uses the entire dataset in both stages. \n",
    "\n",
    "Thus, when an observation arrives on the data stream, the online approach uses this sample to evaluate the model and subsequently to adjust the model weights accordingly.  This process is repeated continually for the entire stream, making the time-order of samples vitally important.  Consider contrasting  this with traditional ML,  for which data splits are often performed together with random mixing for reducing statistical correlations.   This time-ordering assures that the simulated scenario is the same as that encountered in the wild.   It is also vital to uncover the causal structure of the underlying information which could necessitate adjustments for correct future predictions.  \n",
    "\n",
    "\n",
    "\n",
    "### Concept Drift\n",
    "\n",
    "In the context of data streams, the most important reason why traditional ML fails (or at least is not as appropriate, or requires complicated custom re-training processes) is due to “concept drift”.  This term refers to the situation by which data demonstrate fundamental changes over time that render previously learned models obsolete.  It is a challenging problem precisely because long-term causal structure must be captured by the model, while short-term behavior must be interpreted sufficiently rapidly within these changing contexts. \n",
    "\n",
    "While the problem of “concept drift” is also  challenging for “online” approaches,  the advantage is that the weight adjustments to underlying changes in the information on a data stream is continual and does not require ad-hoc methods. In this way, the online methods are self-consistent.  In terms of model adaptation, online methods inherently treat \"concept drift\" and there is no need to retrain the underlying models.\n",
    "\n",
    "Nonetheless,  this remains an active field that we shall revisit with greater depth later in this section of the course.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f324f102",
   "metadata": {},
   "source": [
    "# Stream Machine Learning Libraries\n",
    "\n",
    "Since stream learning is a relatively new method, there are still few implementations. As mentioned in the introduction,  implementations in production are still often ad-hoc and specific to a particular problem and entity. Recently, however, some libraries have become available that contain many of the characteristics required for a production level stream processing system. \n",
    "\n",
    "A summary of some of the principal libraries are the following: \n",
    "\n",
    "- **[Apache SAMOA](https://incubator.apache.org/projects/samoa.html)**, a project to perform analysis and data mining on data streams. There are specific modules for machine learning. Nonetheless,  the project has not been updated since 2020 and remains in the  incubator of the Apache Foundation (AP). ~~Also, it is rumored that the AP has plans to drop the development for this project.~~ The SAMOA podling was finally retired.\n",
    "\n",
    "- **[MOA](https://moa.cms.waikato.ac.nz/)**, the name comes from Massive Online Analysis. This project was developed by the same authors of the WEKA project, written in Java, and with deep ties to that project. It includes a collection of machine learning algorithms (classification, regression, clustering, outlier detection, concept drift detection and recommender systems) and tools for evaluation. It is mainly limited to the interface provided or to implement extensions if you want to work with the remaining ecosystem.\n",
    "\n",
    "- **[Vowpal wabbit](https://vowpalwabbit.org/)**, a Python library sponsored by Microsoft.  It also has tools that are focused on stream learning, however with a particular focus on problems in  reinforcement learning.  A significant disadvantage is that it requires a specific input data format, thereby greatly limiting its usability.   \n",
    "\n",
    "- **[River](https://riverml.xyz/)**, a Python library focused on stream learning and that has adopted a more general approach to problems, as compared to Vowpal Wabbit. In this library,  the number of models is similar to those in scikit-learn, together with tools that interface to the scikit-learn ecosystem.  Additionally,  it also has the possibility to develop reinforcement learning approaches. As a main advantage, it can work with the most common types of data such as Pandas Dataframes.\n",
    "\n",
    "In this class, we will use the River library.  \n",
    "\n",
    "By using River, we shall describe several user cases and uncover many of the common issues within stream or incremental learning. \n",
    "\n",
    "The rest of this notebook shall investigate a few common examples using River.  \n",
    "\n",
    "The River code repository is available in the following [link](https://github.com/online-ml/river/tree/main/river)\n",
    "\n",
    "The River API reference is available in the following [link](https://riverml.xyz/latest/api/overview/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "205a5777",
   "metadata": {},
   "source": [
    "## Binary classisification\n",
    "\n",
    "This is perhaps the most elemental approach to machine learning, and provides a good starting point to discuss stream learning.   In this case, the model has a single output that describes which of the two classes  the sample belongs. \n",
    "\n",
    "1. First step:  install the library (if it is not installed, the pip command is called).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e7893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It would required to use a version of Python >3.8\n",
    "try:\n",
    "    import river\n",
    "except ImportError as err:\n",
    "    !pip install river\n",
    "\n",
    "    \n",
    "# this library is only to improve the redability of some structures\n",
    "# https://rich.readthedocs.io/en/stable/introduction.html\n",
    "try:\n",
    "    from rich import print\n",
    "except ImportError as err:\n",
    "    !pip install rich\n",
    "    from rich import print"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4288d5f7",
   "metadata": {},
   "source": [
    "2. Import a dataset. For this example,  we choose a dataset that is used to train a model in order to detect bank fraud with credit cards.  This [dataset](https://riverml.xyz/latest/api/datasets/CreditCard/) is part of the river example dataset and is easy to load.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from river import datasets\n",
    "\n",
    "dataset = datasets.CreditCard()\n",
    "print(f\"The object contains the information of the dataset, such as, number of samples and features\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look on the first example\n",
    "sample, target = next(iter(dataset)) #An interator is created from the dataset and the first item is obtained\n",
    "print(sample)\n",
    "print(target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5e8d48a",
   "metadata": {},
   "source": [
    "Working with imbalanced classes is quite a usual situation in online learning for tasks such as fraud detection and spam classification. As seen here,  the CreditCard dataset is certainly unbalanced and thus,  it provides us with information about its classes in the description. However, we can easily calculate the percent representation of data within each class: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections #python library\n",
    "\n",
    "counts = collections.Counter(target for _, target in dataset)#it generates a dictionary with labels and counts\n",
    "\n",
    "for label, count in counts.items():\n",
    "    print(f'{label}: {count} ({count / sum(counts.values()):.2%})')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dd4bf02",
   "metadata": {},
   "source": [
    "In this example, we are focusing on the stream method and do not address the imbalanced problem. Nonetheless, there are several approaches to deal with such problems in order to improve the ML model prediction performance. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31abb124",
   "metadata": {},
   "source": [
    "3. Build a model that can be used to discriminate between the two classes. In this particular case, a very simple linear_model ([logisticRegression](https://riverml.xyz/latest/api/linear-model/LogisticRegression/)) is going to be created in order to exemplify a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import linear_model\n",
    "\n",
    "model = linear_model.LogisticRegression()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "036c1fe5",
   "metadata": {},
   "source": [
    "Without properly training the model, the result of the probabilities for each class is exactly the same as it can be seen on the call to function `predict_proba_one`. Let's see which response we have with the previous `sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0fb760",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict_proba_one(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66530209",
   "metadata": {},
   "source": [
    "For each class,  we have a random classifier without any knowledge.  Here is where the method differs from traditional machine learning.  Using the same sample that was used for testing,  shall also be used to adjust the model.  **Note**, any performance metric should  be calculated before adjusting the model.\n",
    "\n",
    "4. Train the model with the present sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn_one(sample, target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "015c33ac",
   "metadata": {},
   "source": [
    "If we test again with the same pattern we would see variation of the probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153346d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict_proba_one(sample))\n",
    "\n",
    "#We are using the same example for learning and, afterward, for prediction again with it. \n",
    "#However, this is neither correct nor fair; it is solely done for academic purposes.\n",
    "#The correct sequence for each sample should be: prediction - evaluation - train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d70980bd",
   "metadata": {},
   "source": [
    "To test the output,  execute the following function: <code>predict_one()</code>, which returns the class label without probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82663a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict_one(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a1ceef8",
   "metadata": {},
   "source": [
    "To integrate the steps in a single loop and see a complete process, the following piece of code shows how to use a loop and how to integrate a rolling measure for this type of system. Several other metrics are also available in River. In this case,  we use the standard metric consisting of the area under the [ROC curve](https://riverml.xyz/latest/api/metrics/ROCAUC/).  Nonetheless, we could have selected any other metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "metric = metrics.ROCAUC()\n",
    "\n",
    "for sample, target in dataset:\n",
    "    prediction = model.predict_proba_one(sample)\n",
    "    metric.update(target, prediction)\n",
    "    model.learn_one(sample, target)\n",
    "   \n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5910867",
   "metadata": {},
   "source": [
    "A common and simple approach to improve the model performance is to scale the data. There are different preprocessing operations available in River including methods for scaling data. One approach is the data standardization using the [preprocessing.StandardScaler](https://riverml.xyz/latest/api/preprocessing/StandardScaler/). \n",
    "\n",
    "The integration with  `scikit-learn` is a powerful feature of River.   Not only can models be wrapped to behave in a similar way to `scikit-learn`, but the pipelines object (which we will discuss later)  provides facilities to link different processes. For example, here is a pipeline with two operators: StandardScaler and LogisticRegression. \n",
    "\n",
    "Also, in this example, we didn’t write an explicit loop because the built-in function , `evaluate. progressive_val_score`,  performs this internally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d9503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import evaluate, compose, preprocessing\n",
    "\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LogisticRegression()\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "metric = metrics.ROCAUC()\n",
    "evaluate.progressive_val_score(dataset, model, metric)\n",
    "\n",
    "#progressive_val_score is equivalent to:\n",
    "#for sample, target in dataset:\n",
    "#    prediction = model.predict_proba_one(sample)\n",
    "#    metric.update(target, prediction)\n",
    "#    model.learn_one(sample, target)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b26c3662",
   "metadata": {},
   "source": [
    "### Play with River\n",
    "\n",
    "The selected metric used to evaluate a model that work with imbalanced classes is critical. **You can try to evaluate the model using the Accuracy metric** (it is part of the River API). You will get impressive metrics even without scaling the data!!! A simple model that always predict \"non-fraud\" would get a high accuracy because \"non-fraud\" is the majority class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline using the Accuracy metric\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60ced8b7",
   "metadata": {},
   "source": [
    "The Cohen's Kappa coefficient is a useful metric to evaluate models with imbalanced classes. This coefficient measures the agreement between the desired label and the label given by the model output excluding the probability of agreement by chance.  This metric is commonly considered more robust than the accuracy and its value is usually lower.\n",
    "The Cohen's Kappa metric is also available in River. \n",
    "\n",
    "**Try to evaluate the model using this metric. Check how the metric changes using the standardization.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478377a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline using the Cohen's Kappa metric\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d4777bb",
   "metadata": {},
   "source": [
    "What about other models? **Try to repeat the pipeline with another one**. For instance you can check the [Perceptron](https://riverml.xyz/latest/api/linear-model/Perceptron/) linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb46f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline using another model \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efcae40f",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "\n",
    "Using stream learning to perform multiclass classification is the next step of complexity we consider.  In this case,  the data sample could belong to one of many other unique classes (with their associated label).   \n",
    "\n",
    "For this case, the steps for implementing stream learning are similar to those employed in  binary classification,  with the difference of modifying the loss functions to take into account the multiple outputs. \n",
    "\n",
    "In the following example, we use another River dataset that consists of a set of images that represent 7 possible classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from river import datasets\n",
    "\n",
    "dataset = datasets.ImageSegments()\n",
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6376c72e",
   "metadata": {},
   "source": [
    "As in binary classification, the dataset consists of the samples associated with a particular target in a tuple-like structure. One difference with our analysis of the binary classifier problem is our choice of classifier. In this case, we employ a new classification method, called the [Hoeffding tree](https://riverml.xyz/latest/api/tree/HoeffdingTreeClassifier/). \n",
    "\n",
    "\n",
    "In the example below, the classifier is loaded with “tree” module.  The specific classifier is instantiated into the object, model.  Next, we print  the class probabilities for a specific sample (<code>predict_proba_one</code>), however, it produces an empty dictionary.   The reason is that the model has not already seen any sample. Therefore, it has no information about the \"possible\" classes. If this were a binary classifier, it would output a probability of 50% for True and False because the classes would be implicit. However,  in this case, we're doing multiclass classification and the output is null.\n",
    "\n",
    "\n",
    "Therefore, the <code>predict_one</code> method initially returns None because the model hasn't seen any labelled data yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107832ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import tree\n",
    "\n",
    "data_stream = iter(dataset)\n",
    "sample, target = next(data_stream)\n",
    "\n",
    "model = tree.HoeffdingTreeClassifier()\n",
    "print(model.predict_proba_one(sample))\n",
    "print(model.predict_one(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb45f2c4",
   "metadata": {},
   "source": [
    "However, after the model learns from examples, it adds those classes to the probabilities of the model. For example, learning the first sample will associate 100% of probability that  the sample belongs to a class.  At this point, no other options are possible, since only one sample was observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6931677",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn_one(sample, target)\n",
    "print(model.predict_proba_one(sample))\n",
    "print(model.predict_one(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1aa28b04",
   "metadata": {},
   "source": [
    "If a second sample is used to train, we can see how the probabilies change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e91e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target = next(data_stream) # Next sample on the list\n",
    "\n",
    "model.learn_one(sample, target)\n",
    "print(model.predict_proba_one(sample))\n",
    "print(model.predict_one(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6a3517c",
   "metadata": {},
   "source": [
    "This is one of the key points of online classifiers:  the models can deal with new classes which appear in the data stream.\n",
    "\n",
    "Typically, the data is used once to make a prediction. When the prediction is made, the ground-truth will emerge later and it can be used first to train the model and also to evaluate. This schema is usually called **progressive validation**. Once the model is evaluated, the same observation is used to adjust the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd262130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "\n",
    "model = tree.HoeffdingTreeClassifier()\n",
    "\n",
    "metric = metrics.ClassificationReport()\n",
    "\n",
    "for sample, target in dataset:\n",
    "    prediction = model.predict_one(sample)\n",
    "    if prediction is not None:# The first iteration, the prediction is None\n",
    "        metric.update(target, prediction)\n",
    "    model.learn_one(sample, target)\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48b1f532",
   "metadata": {},
   "source": [
    "In this case, [ClassificationReport](https://riverml.xyz/latest/api/metrics/ClassificationReport/) retrieves the precision, recall, and F1 for each class that the model has seen. Additionally, the Support column indicates the number of instances identified in the stream. Finally, the function calculates and prints the three different aggregated measures together with the general accuracy of the system. \n",
    "\n",
    "This example demonstrates  a typical pipeline in stream learning. It is so frequent that  River has encapsulated the whole process in a single instance, as in the binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14dae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import evaluate\n",
    "\n",
    "model = tree.HoeffdingTreeClassifier()\n",
    "metric = metrics.ClassificationReport()\n",
    "\n",
    "print(evaluate.progressive_val_score(dataset, model, metric))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15c9264c",
   "metadata": {},
   "source": [
    "### Play with River\n",
    "\n",
    "River provides neighbor-based models for multiclass classification. Check the available models and try the appropriate one. Configure the evaluation process to print the metrics every 1,000 observations\n",
    "\n",
    "\n",
    "This is a heavy time consuming process because of the neighbor-based model. You can stop it once you check the first metric results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6af401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try a neighbor-based model for multiclass classification\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfdd8ee2",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "As a final example that is typical of ML problems,  we study regression.  For these types of problems, the ML model must predict a numerical output given a particular sample that represents the evolution of a time-series.  \n",
    "\n",
    "A  regression sample consists of several features and a target,  which is usually encoded as a continuous number (although it may also be discrete).   A useful example, included in the River library,  is the Trump approval rating dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff884e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "\n",
    "dataset = datasets.TrumpApproval()\n",
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de8d757d",
   "metadata": {},
   "source": [
    "As seen above,  each sample has 6 features that are used to make a prediction in $[0,1]$. For this problem,  we shall use a regression model.  In particular, we use  an adapted [KNN](https://riverml.xyz/0.14.0/api/neighbors/KNNRegressor/) that is included in the River  library.\n",
    "\n",
    "Note that the regression models do not have the <code>predict_proba_one()</code> method,  since it does not calculate class probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1377fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import neighbors\n",
    "\n",
    "data_stream = iter(dataset)\n",
    "sample, target = next(data_stream)\n",
    "\n",
    "model = neighbors.KNNRegressor()\n",
    "print(model.predict_one(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4a3c6d2",
   "metadata": {},
   "source": [
    "As it can be seen, the model has not been trained already and, therefore, the default output is $0.0$. Now, we are going to train the model and repeat the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d8b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn_one(sample, target)\n",
    "print(model.predict_one(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f355c53",
   "metadata": {},
   "source": [
    "Utilizing **progressive validation**,  as in the previous cases, we can employ the same sequence of operations:   prediction, evaluation,  and train that we have used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e6cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "\n",
    "model = neighbors.KNNRegressor()\n",
    "\n",
    "metric = metrics.MAE()\n",
    "\n",
    "for sample, target in dataset:\n",
    "    prediction = model.predict_one(sample)\n",
    "    metric.update(target, prediction)\n",
    "    model.learn_one(sample, target)\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "339df89d",
   "metadata": {},
   "source": [
    "Or, in the compact notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import evaluate\n",
    "\n",
    "model = neighbors.KNNRegressor()\n",
    "metric = metrics.MAE()\n",
    "\n",
    "evaluate.progressive_val_score(dataset, model, metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfd21f7f",
   "metadata": {},
   "source": [
    "### Play with River\n",
    "\n",
    "It's important to highlight that models relying on distance metrics are highly sensitive to variations in feature scales. Establish a preprocessing pipeline for the datasets to standardize the data, and then reassess the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bebaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try a pipeline with a standardization preprocessing step and a KNN model\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
